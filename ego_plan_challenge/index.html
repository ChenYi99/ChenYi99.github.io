<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="EgoPlan Challenge">
  <meta name="keywords" content="EgoPlan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EgoPlan Challenge</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
<!--  <link rel="icon" href="./static/images/favicon.svg">-->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">EgoPlan Challenge@ICML 2024</h1>
          <h5 class="title is-5 publication-title"><a href="https://icml-mfm-eai.github.io/challenges/#TRACK1">
            ICML 2024 WORKSHOP: Multi-modal Foundation Model meets Embodied AI Challenges - EgoPlan Track</a></h5>
          <hr/>
<!--          <div class="is-size-5 publication-authors">-->
<!--            <span class="author-block">-->
<!--              <a>Yi Chen</a><sup>1,2</sup>,</span>-->
<!--            <span class="author-block">-->
<!--              <a href="https://geyuying.github.io/">Yuying Ge</a><sup>1</sup>,</span>-->
<!--            <span class="author-block">-->
<!--              <a href=https://geyixiao.com/>Yixiao Ge</a><sup>1,3</sup>,-->
<!--            </span>-->
<!--            <span class="author-block">-->
<!--              <a href="https://dingmyu.github.io/">Mingyu Ding</a><sup>4</sup>,-->
<!--            </span>-->
<!--            <span class="author-block">-->
<!--              <a href="https://bohao-lee.github.io/">Bohao Li</a><sup>1</sup>,-->
<!--            </span>-->
<!--            <span class="author-block">-->
<!--              <a>Rui Wang</a>,-->
<!--            </span>-->
<!--            <br>-->
<!--            <span class="author-block">-->
<!--              <a>Ruifeng Xu</a><sup>5</sup>,-->
<!--            </span>-->
<!--            <span class="author-block">-->
<!--              <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en">Ying Shan</a><sup>1,3</sup>,-->
<!--            </span>-->
<!--            <span class="author-block">-->
<!--              <a href="https://xh-liu.github.io/">Xihui Liu</a><sup>2</sup>-->
<!--            </span>-->
<!--          </div>-->

<!--          <div class="is-size-5 publication-authors">-->
<!--            <span class="author-block"><sup>1</sup>Tencent AI Lab,</span>-->
<!--            <span class="author-block"><sup>2</sup>The University of Hong Kong,</span>-->
<!--            <span class="author-block"><sup>3</sup>ARC Lab, Tencent PCG,</span> <br>-->
<!--            <span class="author-block"><sup>4</sup>University of California, Berkeley,</span>-->
<!--&lt;!&ndash;            <span class="author-block"><sup>4</sup>The Chinese University of Hong Kong, Shenzhen,</span>&ndash;&gt;-->
<!--            <span class="author-block"><sup>5</sup>Peng Cheng Laboratory</span>-->
<!--          </div>-->

<!--          <div class="column has-text-centered">-->
<!--            <div class="publication-links">-->
<!--              <span class="link-block">-->
<!--                <a href="https://arxiv.org/abs/2312.06722"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="ai ai-arxiv"></i>-->
<!--                  </span>-->
<!--                  <span>arXiv</span>-->
<!--                </a>-->
<!--              </span>-->
<!--              <span class="link-block">-->
<!--                <a href="https://github.com/ChenYi99/EgoPlan/tree/challenge"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fab fa-github"></i>-->
<!--                  </span>-->
<!--                  <span>Code</span>-->
<!--                  </a>-->
<!--              </span>-->
<!--              &lt;!&ndash; Dataset Link. &ndash;&gt;-->
<!--              <span class="link-block">-->
<!--                <a href="https://drive.google.com/drive/folders/1qVtPzhHmCgdQ5JlMeAL3OZtvbHaXktTo?usp=sharing"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="far fa-images"></i>-->
<!--                  </span>-->
<!--                  <span>Data</span>-->
<!--                </a>-->
<!--              </span>-->
<!--            </div>-->
<!--          </div>-->
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Challenge overview. -->
<div class="columns is-centered has-text-centered">
  <div class="container is-max-desktop content">
    <img src="./static/images/challenge_overview.png"
         alt="Challenge overview."/>
  </div>
</div>

<section class="section">
  <div class="container is-max-desktop">
      <h2 class="title is-3">üöÄIntroduction</h2>
      <div class="content has-text-justified">
      <p>
      The EgoPlan Challenge will be held at <a href="https://icml-mfm-eai.github.io/">ICML 2024 WORKSHOP: Multi-modal Foundation Model meets Embodied AI</a>.
      </p>

      <p>
      Embodied task planning in real-world scenarios presents significant challenges, as it requires a comprehensive understanding of the dynamic and complicated visual environment and the open-form task goals.  Multimodal Large Language Models (MLLMs), combining the remarkable reasoning and generalization capabilities of Large Language Models with the ability to comprehend visual inputs, have opened up new possibilities for embodied task planning.
      </p>

      <p>
      The EgoPlan Challenge aims to evaluate the planning capabilities of MLLMs in complex real-world scenarios, focusing on realistic tasks involved in human daily activities. In the competition, models need to choose the most reasonable next step from a diverse set of candidate actions based on open-form task goal descriptions, real-time task progress videos, and current environment observations, to effectively advance task completion.
      </p>

      <p>
      We encourage participants to explore MLLM-related technologies (including but not limited to instruction tuning, prompt engineering, etc.) to enhance the task planning capabilities of MLLMs, thereby promoting the research and application of MLLMs as versatile AI assistants in daily life.
      </p>
      </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">üîçTrack Information</h2>
    <div class="content has-text-justified" style="text-indent: 1em;">
      <p><strong>&#x2022; Challenge Website</strong></p>
      <p><a href="https://icml-mfm-eai.github.io/challenges/#TRACK1">  https://icml-mfm-eai.github.io/challenges/#TRACK1</a></p>

      <p><strong>&#x2022; GitHub Repository</strong></p>
      <p><a href="https://github.com/ChenYi99/EgoPlan/tree/challenge"> https://github.com/ChenYi99/EgoPlan/tree/challenge</a></p>

      <p><strong>&#x2022; EgoPlan Paper</strong></p>
      <p><a href="https://arxiv.org/abs/2312.06722">[2312.06722] EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with Multimodal Large Language Models</a></p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">üìÜTimeline</h2>
    <div class="content has-text-justified" style="text-indent: 1em;">
      <p><strong>&#x2022; From now until July 1, 2024:</strong> Register for this challenge by filling out the
        <a href="https://docs.google.com/forms/d/e/1FAIpQLScnWoXjZcwaagozP3jXnzdSEXX3r2tgXbqO6JWP_lr_fdnpQw/viewform?usp=sf_link">Google Form</a></p>
      <p><strong>&#x2022; May 1, 2024:</strong> Training set and validation set available</p>
      <p><strong>&#x2022; June 1, 2024:</strong> Test set available, test server opens</p>
      <p><strong>&#x2022; July 1, 2024:</strong> Test server closes, registration ends</p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">üìöDataset Information</h2>
    <p>The EgoPlan datasets are constructed based on the two existing egocentric video sources: Epic-Kitchens-100[1] and Ego4D[2].</p>
    <br>

    <div class="content has-text-justified" style="text-indent: 1em;">
      <p>&#x2022; The training dataset is automatically constructed and encompasses 50K instruction-following pairs.</p>

      <p>&#x2022; The validation set contains 3,355 human-verified multiple-choice questions with ground-truth answers.</p>

      <p>&#x2022; The test set will be released on June 1, 2024. Please follow the challenge website and the GitHub repository for updates.</p>
    </div>
    <p>For more details, please refer to the
        <a href="https://github.com/ChenYi99/EgoPlan/tree/challenge?tab=readme-ov-file#data">GitHub repository</a>.</p>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">üìäEvaluation Metrics</h2>
    <div class="content has-text-justified">
      <p>
        Questions are formatted as multiple-choice problems. MLLMs need to select the most reasonable answer from four candidate choices.
        The primary metric is  Accuracy.
      </p>
    </div>
    <img src="./static/images/question_formulation.png"
         alt="Question formulation."/>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">üñ•Ô∏èTest Server</h2>
    <div class="content has-text-justified">
      <p>The test server will be open on June 1, 2024. Please follow the challenge website and the GitHub repository for updates.</p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">üìàBaselines</h2>
    <div class="content has-text-justified">
      <p>
        We enhance the planning capability of Video-LLaMA[3] by instruction-tuning on our training data.
        The detailed method can be referred to in Section 5 of the EgoPlan paper, and the implementation details can can be found in the <a href="https://github.com/ChenYi99/EgoPlan/tree/challenge?tab=readme-ov-file#5-training">GitHub repository</a>.
      </p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">‚öôÔ∏èReference Training Cost</h2>
    <div class="content has-text-justified">
      <p>
        8 V100 cards for 0.5 days.
      </p>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">üìùParticipation</h2>
    <div class="content has-text-justified">
      <p>
        From now until July 1, 2024, participants can register for this challenge by filling out the
        <a href="https://docs.google.com/forms/d/e/1FAIpQLScnWoXjZcwaagozP3jXnzdSEXX3r2tgXbqO6JWP_lr_fdnpQw/viewform?usp=sf_link">Google Form</a>.
      </p>

      <p>
        After the test set is released, results can be submitted via the test server since June 1, 2024.
      </p>

      <p>
        Please follow the challenge website and the GitHub repository for updates.
        We will also keep you updated on the challenge news through the email address you provided in the Google Form.
      </p>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">üèÖAward</h2>
    <div class="content has-text-justified" style="text-indent: 1em;">
      <p><strong>&#x2022; Outstanding Champion:</strong> USD $800</p>

      <p><strong>&#x2022; Honorable Runner-up:</strong> USD $600</p>

      <p><strong>&#x2022; Innovation Award:</strong> USD $600</p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">‚ùóRules</h2>
    <div class="content has-text-justified">
      <ul>
        <li>For participation in the challenge, it is a strict requirement to register for your team by filling out the
        <a href="https://docs.google.com/forms/d/e/1FAIpQLScnWoXjZcwaagozP3jXnzdSEXX3r2tgXbqO6JWP_lr_fdnpQw/viewform?usp=sf_link">Google Form</a>.</li>
        <li>Any kind of Multimodal Large Language Model can be used in this challenge.</li>
        <li>During inference, the visual input should only contain the current observation frame and the preceding frames. No future frame is allowed.</li>
        <li>Using training data in addition to the officially released EgoPlan-IT is allowed.</li>
        <li>In order to check for compliance, we will ask the participants to provide technical reports to the challenge committee and participants will be asked to provide a public talk about their works after winning the award.</li>
      </ul>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">üì©Contact</h2>
    <div class="content has-text-justified" style="text-indent: 1em;">
      <p>&#x2022; GitHub Issue (<a href="https://github.com/ChenYi99/EgoPlan/issues">Issues ¬∑ ChenYi99/EgoPlan (github.com)</a>)</p>
      <p>&#x2022; yichennlp@gmail.com</p>
      <p>&#x2022; <a href="https://join.slack.com/t/icml2024mfmea-yvg5436/shared_invite/zt-2hxdqu2xg-A0M_NVmg0t8r0ZtxbjliPg">Slack channel: #egoplan-challenge-2024</a></p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">üë•Organizers</h2>
    <div class="container is-max-desktop content">
    <img src="./static/images/organizers.png"
         alt="Challenge overview."/>
  </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">References</h2>
    <div class="content has-text-justified">
      <p>[1] Damen, Dima, et al. "Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100." International Journal of Computer Vision(2022): 1-23.</p>

      <p>[2] Grauman, Kristen, et al. "Ego4d: Around the world in 3,000 hours of egocentric video." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.</p>

      <p>[3] Zhang, Hang, Xin Li, and Lidong Bing. "Video-llama: An instruction-tuned audio-visual language model for video understanding." arXiv preprint arXiv:2306.02858 (2023).</p>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{chen2023egoplan,
  title={EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with Multimodal Large Language Models},
  author={Chen, Yi and Ge, Yuying and Ge, Yixiao and Ding, Mingyu and Li, Bohao and Wang, Rui and Xu, Ruifeng and Shan, Ying and Liu, Xihui},
  journal={arXiv preprint arXiv:2312.06722},
  year={2023}
}</code></pre>
  </div>
</section>


</body>
</html>
