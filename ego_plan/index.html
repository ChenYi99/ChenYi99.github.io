<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with Multimodal Large Language Models">
  <meta name="keywords" content="EgoPlan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with Multimodal Large Language Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
<!--  <link rel="icon" href="./static/images/favicon.svg">-->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!--<nav class="navbar" role="navigation" aria-label="main navigation">-->
<!--  <div class="navbar-brand">-->
<!--    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">-->
<!--      <span aria-hidden="true"></span>-->
<!--      <span aria-hidden="true"></span>-->
<!--      <span aria-hidden="true"></span>-->
<!--    </a>-->
<!--  </div>-->
<!--  <div class="navbar-menu">-->
<!--    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">-->
<!--      <a class="navbar-item" href="https://keunhong.com">-->
<!--      <span class="icon">-->
<!--          <i class="fas fa-home"></i>-->
<!--      </span>-->
<!--      </a>-->

<!--      <div class="navbar-item has-dropdown is-hoverable">-->
<!--        <a class="navbar-link">-->
<!--          More Research-->
<!--        </a>-->
<!--        <div class="navbar-dropdown">-->
<!--          <a class="navbar-item" href="https://hypernerf.github.io">-->
<!--            HyperNeRF-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://nerfies.github.io">-->
<!--            Nerfies-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://latentfusion.github.io">-->
<!--            LatentFusion-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://photoshape.github.io">-->
<!--            PhotoShape-->
<!--          </a>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->

<!--  </div>-->
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with
Multimodal Large Language Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a>Yi Chen</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://geyuying.github.io/">Yuying Ge</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href=https://geyixiao.com/>Yixiao Ge</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://dingmyu.github.io/">Mingyu Ding</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://bohao-lee.github.io/">Bohao Li</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a>Rui Wang</a>,
            </span>
            <br>
            <span class="author-block">
              <a>Ruifeng Xu</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en">Ying Shan</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://xh-liu.github.io/">Xihui Liu</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tencent AI Lab,</span>
            <span class="author-block"><sup>2</sup>The University of Hong Kong,</span>
            <span class="author-block"><sup>3</sup>ARC Lab, Tencent PCG,</span> <br>
            <span class="author-block"><sup>4</sup>University of California, Berkeley,</span>
<!--            <span class="author-block"><sup>4</sup>The Chinese University of Hong Kong, Shenzhen,</span>-->
            <span class="author-block"><sup>5</sup>Peng Cheng Laboratory</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
<!--              <span class="link-block">-->
<!--                <a href="https://arxiv.org/pdf/2011.12948"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fas fa-file-pdf"></i>-->
<!--                  </span>-->
<!--                  <span>Paper</span>-->
<!--                </a>-->
<!--              </span>-->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2312.06722"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
<!--              <span class="link-block">-->
<!--                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fab fa-youtube"></i>-->
<!--                  </span>-->
<!--                  <span>Video</span>-->
<!--                </a>-->
<!--              </span>-->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ChenYi99/EgoPlan"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/drive/folders/1OUnQzG79kxhJdaquBKLv1rrKz36TTkP6?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Benchmark overview. -->
<div class="columns is-centered has-text-centered">
  <div class="container is-max-desktop content">
    <img src="./static/images/benchmark_overview.png"
         alt="Benchmark overview."/>
    <div class="content has-text-justified">
        Our EgoPlan-Bench evaluates Egocentric Embodied <b>Planning</b>, where a model predicts the next feasible action by taking a video
        showing task progress, current visual observation and language instruction as inputs. In contrast, existing egocentric video QA benchmarks
        mainly evaluate <b>Comprehension</b>, where a model answers questions based on the spatial and temporal understanding of the entire video.
        The key difference is that the setting of EgoPlan-Bench better aligns with real-world embodied AI applications, as the modelâ€™s output can
        directly serve as plans for agents to execute tasks in real environments.
    </div>
  </div>
</div>
<!--/ Benchmark overview. -->

<!--<section class="hero teaser">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="hero-body">-->
<!--      <video id="teaser" autoplay muted loop playsinline height="100%">-->
<!--        <source src="./static/videos/teaser.mp4"-->
<!--                type="video/mp4">-->
<!--      </video>-->
<!--      <h2 class="subtitle has-text-centered">-->
<!--        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into-->
<!--        free-viewpoint-->
<!--        portraits.-->
<!--      </h2>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
<!--          <p>-->
<!--            Multimodal Large Language Models (MLLMs), building upon the powerful Large Language Models (LLMs)-->
<!--            with exceptional reasoning and generalization capability,-->
<!--            have opened up new avenues for embodied task planning.-->
<!--            MLLMs excel in their ability to integrate diverse environmental inputs, such as real-time task progress, visual observations, and open-form language instructions, which are-->
<!--            crucial for executable task planning.-->
<!--          </p>-->

<!--          <p>-->
<!--            In this work, we introduce a benchmark with human annotations, <b>EgoPlan-Bench</b>, to quantitatively investigate the potential of MLLMs-->
<!--            as embodied task planners in real-world scenarios. Our-->
<!--            benchmark is distinguished by realistic tasks derived from-->
<!--            real-world videos, a diverse set of actions involving interactions with hundreds of different objects, and complex visual observations from varied environments. We evaluate-->
<!--            various open-source MLLMs, revealing that these models-->
<!--            have not yet evolved into embodied planning generalists-->
<!--            (even GPT-4V).-->
<!--          </p>-->

<!--          <p>-->
<!--            We further construct an instruction-tuning-->
<!--            dataset <b>EgoPlan-IT</b> from videos of human-object interactions, to facilitate the learning of high-level task planning-->
<!--            in intricate real-world situations. The experiment results-->
<!--            demonstrate that the model tuned on EgoPlan-IT not only-->
<!--            significantly improves performance on our benchmark, but-->
<!--            also effectively acts as embodied planner in simulations.-->
<!--          </p>-->

          <p>
          Multimodal Large Language Models, combining the remarkable reasoning and generalization capabilities of Large Language Models
          (LLMs) with the ability to comprehend visual inputs, have opened up
          new avenues for embodied task planning. Given diverse environmental
          inputs, including real-time task progress, visual observations, and open-form language instructions, a proficient task planner is expected to predict feasible actions, which is a feat inherently achievable by Multimodal
          Large Language Models (MLLMs).
          </p>

          <p>
          In this work, we aim to quantitatively investigate the potential of MLLMs as embodied task planners
          in real-world scenarios by introducing a benchmark with human annotations named <b>EgoPlan-Bench</b>. Our benchmark is distinguished by realistic
          tasks derived from real-world videos, a diverse set of actions involving
          interactions with hundreds of different objects, and complex visual observations from varied scenes. We evaluate a wide range of MLLMs, revealing that these models have not yet evolved into embodied planning
          generalists (even GPT-4V).
          </p>

          <p>
          We further construct an instruction-tuning
          dataset <b>EgoPlan-IT</b> from videos with human-object interactions, to facilitate the learning of high-level task planning in intricate real-world
          situations. The experiment results demonstrate that the model tuned on
          EgoPlan-IT not only significantly improves performance on our benchmark, but can also be applied as a task planner for guiding embodied
          agents in simulations.
          </p>

        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
<!--    <div class="columns is-centered has-text-centered">-->
<!--      <div class="column is-four-fifths">-->
<!--        <h2 class="title is-3">Video</h2>-->
<!--        <div class="publication-video">-->
<!--          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"-->
<!--                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Benchmark Construction</h2>
    <img src="./static/images/benchmark_construction.png"
         alt="Benchmark construction."/>
    <p>
<!--      Overview of the construction pipeline for EgoPlan-Bench based on existing untrimmed egocentric videos with detailed action-->
<!--      narrations. (1) We first leverage GPT-4 to identify and decompose the common task goals of actions in a hierarchical way. (2) We then-->
<!--      filter out tasks that are either too short or too long. (3) The questions are designed in the form of multiple-choice, where the questions are-->
<!--      automatically generated based on task goals, and the options are derived from actions in different steps of the same task. (4) We finally-->
<!--      employ human annotators to verify each question to ensure the quality of our benchmark.-->
      Overview of the construction pipeline for EgoPlan-Bench based on existing
      untrimmed egocentric videos with detailed action narrations. (1) We first leverage
      GPT-4 to identify task goals through hierarchical reasoning. (2) We then filter task
      goals based on the requisite number of actions. (3) The questions are designed in the
      form of multiple-choice, where the questions are automatically generated based on task
      goals, and the options are derived from different actions under the same task goal. (4)
      We employ human annotators to verify each question to ensure the benchmark quality.
    </p>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Data Statistics</h2>
<!--    <p>-->
<!--      Our benchmark comprises a total-->
<!--      of 3,355 QA pairs, with 2,432 derived from Epic-Kitchen-->
<!--      egocentric videos and 923 from Ego4D.-->
<!--      Drawing upon the attributes of the utilized egocentric video sources, our benchmark exhibits three main-->
<!--      characteristics. <b>1) Realism of Tasks:</b> The tasks are extrapolated from authentic real-world videos, offering a closer-->
<!--      reflection of daily human needs and showcasing greater variety than artificially constructed tasks.-->
<!--      <b>2) Diversity of Actions:</b> The benchmark involves a diverse set of actions, requiring interaction with hundreds of different objects and-->
<!--      extending beyond basic manipulation skills such as picking-->
<!--      and placing items. <b>3) Intricacy of Visual Observations:</b>-->
<!--      The visual observations come from various, often distinct-->
<!--      real-world environments, where objects vary in appearance,-->
<!--      state, and placement.-->
<!--    </p>-->


    <p>
      Our benchmark comprises a total
      of 3,355 QA pairs.
      Drawing upon the attributes of the utilized egocentric video sources, our benchmark exhibits three main
      characteristics.
      <b>1)Realism of Tasks:</b> The tasks are extrapolated from authentic real-world videos,
      offering a closer reflection of daily human needs and showcasing greater variety
      than artificially constructed tasks. <b>2) Diversity of Actions:</b> The benchmark
      involves a diverse set of actions, requiring interaction with hundreds of different objects and extending beyond basic manipulation skills such as picking and
      placing items. <b>3) Intricacy of Visual Observations:</b> The visual observations
      come across various real-world scenes, where objects vary in appearance, state,
      and placement.
    </p>


    <br>
    <div class="columns is-centered">
      <div class="column">

        <!-- Evaluation Data Statistics. -->
        <div class="content has-text-centered">
          <img src="./static/images/data_statistics.png" width="78.5%"
         alt="Evaluation Data Statistics."/>
          <p>
            a) Statistics of the evaluation data of EgoPlan-Bench.
          </p>
        </div>
        <!--/ Evaluation Data Statistics. -->
      </div>
    </div>
    <!--/ Matting. -->

    <div class="columns is-centered">
      <div class="column">
        <br>
        <br>
        <!-- Task Goal Distribution. -->
        <div class="content has-text-centered">
          <img src="./static/images/task_goal_distribution.png" width="100%"
         alt="Task Goal Distribution."/>
          <br><br>
          <p>
            b) Wordcloud of verbs in task goals of EgoPlan-Bench.
          </p>
        </div>
        <!--/ Task Goal Distribution. -->
      </div>

      <!-- Action Distribution. -->
      <div class="column">
        <div class="content has-text-justified">
          <img src="./static/images/action_distribution.png" width="100%"
         alt="Action distribution."/>

          <p>
<!--            The top 20 most common root verbs (inner circle) and-->
<!--            their top 8 direct noun objects (outer circle) in the candidate actions from the EgoPlan-Bench (log rescaled).-->
<!--            c) Distribution of the verbs and noun objects in the candidate actions from the EgoPlan-Bench (log rescaled).-->
            c) Top 20 verbs with top 8 related objects in EgoPlan-<br>Bench candidate actions.
          </p>
        </div>
      </div>
      <!--/ Action Distribution. -->


    </div>
    <!--/ Matting. -->

  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Evaluation Results</h2>
      <p>
        We evaluate a wide range of MLLMs. The results indicate that our EgoPlan-Bench
        poses significant challenges for existing MLLMs, and there
        is still a long way to go before these models evolve into
        generalist embodied task planners.
    </p>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <img src="static/images/evaluation_results_1.png" alt="Failure Case 1" width="100%"/>
          <h2 class="title is-6 has-text-centered">
            Failure Type-1: Tendency to predict actions with a higher overlap of words with the task goal.
          </h2>
        </div>
        <div class="item">
          <img src="static/images/evaluation_results_2.png" alt="Failure Case 2" width="100%"/>
          <h2 class="title is-6 has-text-centered">
            Failure Type-1: Tendency to predict actions with a higher overlap of words with the task goal.
          </h2>
        </div>
        <div class="item">
          <img src="static/images/evaluation_results_3.png" alt="Failure Case 3" width="100%"/>
          <h2 class="title is-6 has-text-centered">
            Failure Type-2: Repeating actions that have already been completed in the observed task progress.
          </h2>
        </div>
        <div class="item">
          <img src="static/images/evaluation_results_4.png" alt="Failure Case 4" width="100%"/>
          <h2 class="title is-6 has-text-centered">
            Failure Type-2: Repeating actions that have already been completed in the observed task progress.
          </h2>
        </div>
        <div class="item">
          <img src="static/images/evaluation_results_6.png" alt="Failure Case 5" width="100%"/>
          <h2 class="title is-6 has-text-centered">
            Failure Type-3: Neglecting crucial object states in visual observations.
          </h2>
        </div>
        <div class="item">
          <img src="static/images/evaluation_results_5.png" alt="Failure Case 6" width="100%"/>
          <h2 class="title is-6 has-text-centered">
            Failure Type-3: Neglecting crucial object states in visual observations.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Instruction Tuning. -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Enhancing EgoPlan Capability by Instruction Tuning</h2>
    <p>
      We further construct EgoPlan-IT, an instruction-tuning dataset, to facilitate the learning of high-level
      task planning from human videos. The model tuned on
      EgoPlan-IT not only exhibits a significant performance
      enhancement on our benchmark, but also shows
      potential as a task planner for an embodied agent to complete long-horizon tasks within a simulated environment.
    </p>
    <img src="./static/images/simulation_case.png"
         alt="Simulation case."/>
  </div>
</section>
<!--/ Instruction Tuning. -->


<!--<section class="section">-->
<!--  <div class="container is-max-desktop">-->

<!--    <div class="columns is-centered">-->

<!--      &lt;!&ndash; Visual Effects. &ndash;&gt;-->
<!--      <div class="column">-->
<!--        <div class="content">-->
<!--          <h2 class="title is-3">Visual Effects</h2>-->
<!--          <p>-->
<!--            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect-->
<!--            would be impossible without nerfies since it would require going through a wall.-->
<!--          </p>-->
<!--          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/dollyzoom-stacked.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--      </div>-->
<!--      &lt;!&ndash;/ Visual Effects. &ndash;&gt;-->

<!--      &lt;!&ndash; Matting. &ndash;&gt;-->
<!--      <div class="column">-->
<!--        <h2 class="title is-3">Matting</h2>-->
<!--        <div class="columns is-centered">-->
<!--          <div class="column content">-->
<!--            <p>-->
<!--              As a byproduct of our method, we can also solve the matting problem by ignoring-->
<!--              samples that fall outside of a bounding box during rendering.-->
<!--            </p>-->
<!--            <video id="matting-video" controls playsinline height="100%">-->
<!--              <source src="./static/videos/matting.mp4"-->
<!--                      type="video/mp4">-->
<!--            </video>-->
<!--          </div>-->

<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--    &lt;!&ndash;/ Matting. &ndash;&gt;-->

<!--    &lt;!&ndash; Animation. &ndash;&gt;-->
<!--    <div class="columns is-centered">-->
<!--      <div class="column is-full-width">-->
<!--        <h2 class="title is-3">Animation</h2>-->

<!--        &lt;!&ndash; Interpolating. &ndash;&gt;-->
<!--        <h3 class="title is-4">Interpolating states</h3>-->
<!--        <div class="content has-text-justified">-->
<!--          <p>-->
<!--            We can also animate the scene by interpolating the deformation latent codes of two input-->
<!--            frames. Use the slider here to linearly interpolate between the left frame and the right-->
<!--            frame.-->
<!--          </p>-->
<!--        </div>-->
<!--        <div class="columns is-vcentered interpolation-panel">-->
<!--          <div class="column is-3 has-text-centered">-->
<!--            <img src="./static/images/interpolate_start.jpg"-->
<!--                 class="interpolation-image"-->
<!--                 alt="Interpolate start reference image."/>-->
<!--            <p>Start Frame</p>-->
<!--          </div>-->
<!--          <div class="column interpolation-video-column">-->
<!--            <div id="interpolation-image-wrapper">-->
<!--              Loading...-->
<!--            </div>-->
<!--            <input class="slider is-fullwidth is-large is-info"-->
<!--                   id="interpolation-slider"-->
<!--                   step="1" min="0" max="100" value="0" type="range">-->
<!--          </div>-->
<!--          <div class="column is-3 has-text-centered">-->
<!--            <img src="./static/images/interpolate_end.jpg"-->
<!--                 class="interpolation-image"-->
<!--                 alt="Interpolation end reference image."/>-->
<!--            <p class="is-bold">End Frame</p>-->
<!--          </div>-->
<!--        </div>-->
<!--        <br/>-->
<!--        &lt;!&ndash;/ Interpolating. &ndash;&gt;-->

<!--        &lt;!&ndash; Re-rendering. &ndash;&gt;-->
<!--        <h3 class="title is-4">Re-rendering the input video</h3>-->
<!--        <div class="content has-text-justified">-->
<!--          <p>-->
<!--            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel-->
<!--            viewpoint such as a stabilized camera by playing back the training deformations.-->
<!--          </p>-->
<!--        </div>-->
<!--        <div class="content has-text-centered">-->
<!--          <video id="replay-video"-->
<!--                 controls-->
<!--                 muted-->
<!--                 preload-->
<!--                 playsinline-->
<!--                 width="75%">-->
<!--            <source src="./static/videos/replay.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        &lt;!&ndash;/ Re-rendering. &ndash;&gt;-->

<!--      </div>-->
<!--    </div>-->
<!--    &lt;!&ndash;/ Animation. &ndash;&gt;-->


<!--    &lt;!&ndash; Concurrent Work. &ndash;&gt;-->
<!--    <div class="columns is-centered">-->
<!--      <div class="column is-full-width">-->
<!--        <h2 class="title is-3">Related Links</h2>-->

<!--        <div class="content has-text-justified">-->
<!--          <p>-->
<!--            There's a lot of excellent work that was introduced around the same time as ours.-->
<!--          </p>-->
<!--          <p>-->
<!--            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.-->
<!--          </p>-->
<!--          <p>-->
<!--            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>-->
<!--            both use deformation fields to model non-rigid scenes.-->
<!--          </p>-->
<!--          <p>-->
<!--            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>-->
<!--          </p>-->
<!--          <p>-->
<!--            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.-->
<!--          </p>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--    &lt;!&ndash;/ Concurrent Work. &ndash;&gt;-->

<!--  </div>-->
<!--</section>-->


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{chen2023egoplan,
  title={EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with Multimodal Large Language Models},
  author={Chen, Yi and Ge, Yuying and Ge, Yixiao and Ding, Mingyu and Li, Bohao and Wang, Rui and Xu, Ruifeng and Shan, Ying and Liu, Xihui},
  journal={arXiv preprint arXiv:2312.06722},
  year={2023}
}</code></pre>
  </div>
</section>


</body>
</html>
