<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="EgoPlan-Bench: Benchmarking Multimodal Large Language Models for Human-Level Planning">
  <meta name="keywords" content="EgoPlan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EgoPlan-Bench: Benchmarking Multimodal Large Language Models for Human-Level Planning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
<!--  <link rel="icon" href="./static/images/favicon.svg">-->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!--<nav class="navbar" role="navigation" aria-label="main navigation">-->
<!--  <div class="navbar-brand">-->
<!--    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">-->
<!--      <span aria-hidden="true"></span>-->
<!--      <span aria-hidden="true"></span>-->
<!--      <span aria-hidden="true"></span>-->
<!--    </a>-->
<!--  </div>-->
<!--  <div class="navbar-menu">-->
<!--    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">-->
<!--      <a class="navbar-item" href="https://keunhong.com">-->
<!--      <span class="icon">-->
<!--          <i class="fas fa-home"></i>-->
<!--      </span>-->
<!--      </a>-->

<!--      <div class="navbar-item has-dropdown is-hoverable">-->
<!--        <a class="navbar-link">-->
<!--          More Research-->
<!--        </a>-->
<!--        <div class="navbar-dropdown">-->
<!--          <a class="navbar-item" href="https://hypernerf.github.io">-->
<!--            HyperNeRF-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://nerfies.github.io">-->
<!--            Nerfies-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://latentfusion.github.io">-->
<!--            LatentFusion-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://photoshape.github.io">-->
<!--            PhotoShape-->
<!--          </a>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->

<!--  </div>-->
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">EgoPlan-Bench: Benchmarking Multimodal Large Language Models for Human-Level Planning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a>Yi Chen</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://geyuying.github.io/">Yuying Ge</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href=https://geyixiao.com/>Yixiao Ge</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://dingmyu.github.io/">Mingyu Ding</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://bohao-lee.github.io/">Bohao Li</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a>Rui Wang</a><sup>1</sup>,
            </span>
            <br>
            <span class="author-block">
              <a>Ruifeng Xu</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en">Ying Shan</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://xh-liu.github.io/">Xihui Liu</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tencent AI Lab,</span>
            <span class="author-block"><sup>2</sup>The University of Hong Kong,</span>
            <span class="author-block"><sup>3</sup>ARC Lab, Tencent PCG,</span> <br>
            <span class="author-block"><sup>4</sup>University of California, Berkeley,</span>
<!--            <span class="author-block"><sup>4</sup>The Chinese University of Hong Kong, Shenzhen,</span>-->
            <span class="author-block"><sup>5</sup>Peng Cheng Laboratory</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
<!--              <span class="link-block">-->
<!--                <a href="https://arxiv.org/pdf/2011.12948"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fas fa-file-pdf"></i>-->
<!--                  </span>-->
<!--                  <span>Paper</span>-->
<!--                </a>-->
<!--              </span>-->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2312.06722"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
<!--              <span class="link-block">-->
<!--                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fab fa-youtube"></i>-->
<!--                  </span>-->
<!--                  <span>Video</span>-->
<!--                </a>-->
<!--              </span>-->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ChenYi99/EgoPlan"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/drive/folders/1qVtPzhHmCgdQ5JlMeAL3OZtvbHaXktTo?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-database"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/spaces/ChenYi99/EgoPlan-Bench_Leaderboard"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-trophy"></i>
                  </span>
                  <span>Leaderboard</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Benchmark overview. -->
<div class="columns is-centered has-text-centered">
  <div class="container is-max-desktop content">
    <img src="./static/images/benchmark_overview.png"
         alt="Benchmark overview."/>
    <div class="content has-text-justified">
        Our EgoPlan-Bench evaluates <b>Planning</b>, where a model predicts the next feasible action plan by taking a video showing task progress,
        current visual observation, and open-form task goal as inputs like humans.
        In contrast, the egocentric-video-based QA examples from existing benchmarks mainly evaluate
        <b>Comprehension</b>, where a model answers questions based on the spatial and temporal understanding of the entire video.
    </div>
  </div>
</div>
<!--/ Benchmark overview. -->

<!--<section class="hero teaser">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="hero-body">-->
<!--      <video id="teaser" autoplay muted loop playsinline height="100%">-->
<!--        <source src="./static/videos/teaser.mp4"-->
<!--                type="video/mp4">-->
<!--      </video>-->
<!--      <h2 class="subtitle has-text-centered">-->
<!--        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into-->
<!--        free-viewpoint-->
<!--        portraits.-->
<!--      </h2>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          The pursuit of artificial general intelligence (AGI) has been accelerated by Multimodal Large Language Models (MLLMs),
          which exhibit superior reasoning, generalization capabilities, and proficiency in processing multimodal inputs.
          A crucial milestone in the evolution of AGI is the attainment of human-level planning, a fundamental ability for making informed decisions in complex environments,
          and solving a wide range of real-world problems. Despite the impressive advancements in MLLMs, a question remains: <b>How far are current MLLMs from achieving human-level planning?</b>

          To shed light on this question, we introduce EgoPlan-Bench, a comprehensive benchmark to evaluate the planning abilities of MLLMs in real-world scenarios from an egocentric perspective, mirroring human perception.
          EgoPlan-Bench emphasizes the evaluation of planning capabilities of MLLMs, featuring realistic tasks, diverse action plans, and intricate visual observations.
          Our rigorous evaluation of a wide range of MLLMs reveals that EgoPlan-Bench poses significant challenges,
          highlighting a substantial scope for improvement in MLLMs to achieve human-level task planning.
          To facilitate this advancement, we further present EgoPlan-IT, a specialized instruction-tuning dataset that effectively enhances model performance on EgoPlan-Bench.
          We have made all codes, data, and a maintained benchmark leaderboard available to advance future research.
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
<!--    <div class="columns is-centered has-text-centered">-->
<!--      <div class="column is-four-fifths">-->
<!--        <h2 class="title is-3">Video</h2>-->
<!--        <div class="publication-video">-->
<!--          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"-->
<!--                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Benchmark Construction</h2>
    <img src="./static/images/benchmark_construction.png"
         alt="Benchmark construction."/>
    <p>
<!--      Overview of the construction pipeline for EgoPlan-Bench based on existing untrimmed egocentric videos with detailed action-->
<!--      narrations. (1) We first leverage GPT-4 to identify and decompose the common task goals of actions in a hierarchical way. (2) We then-->
<!--      filter out tasks that are either too short or too long. (3) The questions are designed in the form of multiple-choice, where the questions are-->
<!--      automatically generated based on task goals, and the options are derived from actions in different steps of the same task. (4) We finally-->
<!--      employ human annotators to verify each question to ensure the quality of our benchmark.-->
      Overview of the construction pipeline for EgoPlan-Bench based on existing
      untrimmed egocentric videos with detailed action narrations. (1) We first leverage
      GPT-4 to identify task goals through hierarchical reasoning. (2) We then filter task
      goals based on the requisite number of actions. (3) The questions are designed in the
      form of multiple-choice, where the questions are automatically generated based on task
      goals, and the options are derived from different actions under the same task goal. (4)
      We employ human annotators to verify each question to ensure the benchmark quality.
    </p>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Data Statistics</h2>
<!--    <p>-->
<!--      Our benchmark comprises a total-->
<!--      of 3,355 QA pairs, with 2,432 derived from Epic-Kitchen-->
<!--      egocentric videos and 923 from Ego4D.-->
<!--      Drawing upon the attributes of the utilized egocentric video sources, our benchmark exhibits three main-->
<!--      characteristics. <b>1) Realism of Tasks:</b> The tasks are extrapolated from authentic real-world videos, offering a closer-->
<!--      reflection of daily human needs and showcasing greater variety than artificially constructed tasks.-->
<!--      <b>2) Diversity of Actions:</b> The benchmark involves a diverse set of actions, requiring interaction with hundreds of different objects and-->
<!--      extending beyond basic manipulation skills such as picking-->
<!--      and placing items. <b>3) Intricacy of Visual Observations:</b>-->
<!--      The visual observations come from various, often distinct-->
<!--      real-world environments, where objects vary in appearance,-->
<!--      state, and placement.-->
<!--    </p>-->


    <p>
      The evaluation data of EgoPlan-Bench comprises a total of 4,939 multiple-choice questions,
      which are divided into two subsets: EgoPlan-Val for validation and EgoPlan-Test for testing.
      Our benchmark exhibits three main characteristics:
      <b>1)Realism of Tasks:</b> The tasks are extrapolated from authentic real-world videos,
      offering a closer reflection of daily human needs and showcasing greater variety
      than artificially constructed tasks. <b>2) Diversity of Action Plans:</b> The benchmark
      involves a diverse set of action plans, requiring interaction with hundreds of different objects and extending beyond basic manipulation skills such as picking and
      placing items. <b>3) Intricacy of Visual Observations:</b> The visual observations
      come across various real-world scenes, where objects vary in appearance, state,
      and placement.
    </p>


    <br>
    <div class="columns is-centered">
      <div class="column">

        <!-- Evaluation Data Statistics. -->
        <div class="content has-text-centered">
          <img src="./static/images/data_statistics.png" width="78.5%"
         alt="Evaluation Data Statistics."/>
          <p>
            a) Statistics of the evaluation data of EgoPlan-Bench.
          </p>
        </div>
        <!--/ Evaluation Data Statistics. -->
      </div>
    </div>
    <!--/ Matting. -->

    <div class="columns is-centered">
      <div class="column">
        <br>
        <br>
        <!-- Task Goal Distribution. -->
        <div class="content has-text-centered">
          <img src="./static/images/task_goal_distribution.png" width="90%"
         alt="Task Goal Distribution."/>
          <br><br>
          <p>
            b) Wordcloud of task goals in EgoPlan-Bench questions.
          </p>
        </div>
        <!--/ Task Goal Distribution. -->
      </div>

      <!-- Action Distribution. -->
      <div class="column">
        <div class="content has-text-justified">
          <img src="./static/images/action_distribution.png" width="90%"
         alt="Action distribution."/>
          <p>
            c) Top 20 verbs with top 8 related objects in EgoPlan-<br>Bench candidate action plans.
          </p>
        </div>
      </div>
      <!--/ Action Distribution. -->


    </div>
    <!--/ Matting. -->

  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Evaluation Results</h2>
      <p>
        We evaluate a total of 28 MLLMs on our benchmark. The results indicate that our benchmark poses
        significant challenges for existing MLLMs, and there is still a long way to go before these models
        evolve into human-level task planners.

        We further analyze three main reasons for this limitation, including 1) insufficient integration of visual modality,
        2) omitting key state changes in task progress, and 3) inadequate application of world knowledge.
      </p>

      <div class="columns is-centered">
        <div class="column">
          <br>
          <br>
          <div class="content has-text-justified">
            <img src="./static/images/goal_answer_similarity_impact.png" width="90%"
           alt="Impact of Goal-Answer Similarity."/>
<!--            <br><br>-->
            <p>
              a) Impact of goal-answer similarity on model performance.
            </p>
          </div>
        </div>


        <div class="column">
          <br>
          <br>
          <div class="content has-text-justified">
            <img src="./static/images/progress_len_impact.png" width="90%"
           alt="Impact of Task Progress Length."/>
            <p>
              b) Impact of task progress length on model performance.
            </p>
          </div>
        </div>
      </div>

      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <img src="static/images/evaluation_results_1.png" alt="Failure Type-1" width="100%"/>
          <h2 class="title is-6 has-text-centered">
            Failure Type-1: Insufficient integration of visual modality.
          </h2>
        </div>
        <div class="item">
          <img src="static/images/evaluation_results_4.png" alt="Failure Type-2" width="100%"/>
          <h2 class="title is-6 has-text-centered">
            Failure Type-2: Omitting key state changes in task progress.
          </h2>
        </div>
        <div class="item">
          <img src="static/images/evaluation_results_5.png" alt="Failure Type-3" width="100%"/>
          <h2 class="title is-6 has-text-centered">
            Failure Type-3: Inadequate application of world knowledge.
          </h2>
        </div>

<!--        <div class="item">-->
<!--          <img src="static/images/evaluation_results_1.png" alt="Failure Case 1" width="100%"/>-->
<!--          <h2 class="title is-6 has-text-centered">-->
<!--            Failure Type-1: Tendency to predict actions with a higher overlap of words with the task goal.-->
<!--          </h2>-->
<!--        </div>-->
<!--        <div class="item">-->
<!--          <img src="static/images/evaluation_results_2.png" alt="Failure Case 2" width="100%"/>-->
<!--          <h2 class="title is-6 has-text-centered">-->
<!--            Failure Type-1: Tendency to predict actions with a higher overlap of words with the task goal.-->
<!--          </h2>-->
<!--        </div>-->
<!--        <div class="item">-->
<!--          <img src="static/images/evaluation_results_3.png" alt="Failure Case 3" width="100%"/>-->
<!--          <h2 class="title is-6 has-text-centered">-->
<!--            Failure Type-2: Repeating actions that have already been completed in the observed task progress.-->
<!--          </h2>-->
<!--        </div>-->
<!--        <div class="item">-->
<!--          <img src="static/images/evaluation_results_4.png" alt="Failure Case 4" width="100%"/>-->
<!--          <h2 class="title is-6 has-text-centered">-->
<!--            Failure Type-2: Repeating actions that have already been completed in the observed task progress.-->
<!--          </h2>-->
<!--        </div>-->
<!--        <div class="item">-->
<!--          <img src="static/images/evaluation_results_6.png" alt="Failure Case 5" width="100%"/>-->
<!--          <h2 class="title is-6 has-text-centered">-->
<!--            Failure Type-3: Neglecting crucial object states in visual observations.-->
<!--          </h2>-->
<!--        </div>-->
<!--        <div class="item">-->
<!--          <img src="static/images/evaluation_results_5.png" alt="Failure Case 6" width="100%"/>-->
<!--          <h2 class="title is-6 has-text-centered">-->
<!--            Failure Type-3: Neglecting crucial object states in visual observations.-->
<!--          </h2>-->
<!--        </div>-->
      </div>

<!--      <script-->
<!--        type="module"-->
<!--        src="https://gradio.s3-us-west-2.amazonaws.com/3.27.0/gradio.js"-->
<!--      ></script>-->

<!--      <gradio-app src="https://chenyi99-egoplan-bench-leaderboard.hf.space"></gradio-app>-->

    </div>
  </div>
</section>

<!-- Instruction Tuning. -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Enhancing Human-Level Planning through Instruction Tuning</h2>
    <p>
      Given the suboptimal performance of the evaluated MLLMs on EgoPlan-Bench, we investigate enhancing
      the human-level planning capabilities of MLLMs through instruction-tuning. Specifically, we
      construct an instruction-tuning dataset, EgoPlan-IT, to align MLLMs with real-world needs of task
      planning.
      The model tuned on EgoPlan-IT demonstrates a significant and robust performance improvement on the proposed benchmark,
      verifying the effectiveness of our data.
      <br><br>

<!--      We further construct EgoPlan-IT, an instruction-tuning dataset, to facilitate the learning of high-level-->
<!--      task planning from human videos. The model tuned on-->
<!--      EgoPlan-IT not only exhibits a significant performance-->
<!--      enhancement on our benchmark, but also shows-->
<!--      potential as a task planner for an embodied agent to complete long-horizon tasks within a simulated environment.-->
    </p>
    <p align="center">
    <img src="./static/images/instruction_tuning_results.png"
         alt="Instruction-tuning results."
         width="60%"/>
    </p>
  </div>
</section>
<!--/ Instruction Tuning. -->



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{chen2023egoplan,
  title={EgoPlan-Bench: Benchmarking Multimodal Large Language Models for Human-Level Planning},
  author={Chen, Yi and Ge, Yuying and Ge, Yixiao and Ding, Mingyu and Li, Bohao and Wang, Rui and Xu, Ruifeng and Shan, Ying and Liu, Xihui},
  journal={arXiv preprint arXiv:2312.06722},
  year={2023}
}</code></pre>
  </div>
</section>


</body>
</html>
